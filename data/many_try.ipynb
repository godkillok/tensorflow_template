{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试验了将sequence example 写入tfrecord中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "labels=[2,3,5]\n",
    "keys=[[1,2],[2,5,6],[2,5,6,7,8,9]]\n",
    "\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def make_example(label,key):\n",
    "    \n",
    "    example = tf.train.SequenceExample(\n",
    "        context=tf.train.Features(\n",
    "            feature={\n",
    "            \"label\":tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
    "        }),\n",
    "        feature_lists=tf.train.FeatureLists(\n",
    "            feature_list={\n",
    "            \"feature\":tf.train.FeatureList(feature=[tf.train.Feature(int64_list=tf.train.Int64List(value=[key[i]])) for i in range(len(key))])\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return example.SerializeToString()\n",
    "\n",
    "\n",
    "filename=\"./tmp.tfrecords\"\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "writer = tf.python_io.TFRecordWriter(filename)\n",
    "for k in range(len(keys)):\n",
    "    key=keys[k]\n",
    "    label=labels[k]\n",
    "    ex = make_example(label,key)\n",
    "    writer.write(ex)\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要试验了 sequence example 的contxt 和sequence_features的用法，得出的结论可以将sequence_features在batch内进行padding，但是textcnn是要求在所有样本采用同一个句子长度，因此不行\n",
    "\n",
    "所以只能在做数据处理时候先给padding 到固定长度之后再写入到tfrecord中了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "reader = tf.TFRecordReader()\n",
    "filename_queue = tf.train.string_input_producer([\"./tmp.tfrecords\"],num_epochs=3)\n",
    "_,serialized_example =reader.read(filename_queue)\n",
    "\n",
    "# coord = tf.train.Coordinator()\n",
    "# threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "\n",
    "context_features={\n",
    "    \"label\":tf.FixedLenFeature([],dtype=tf.int64)\n",
    "}\n",
    "sequence_features={\n",
    "    \"feature\":tf.FixedLenSequenceFeature([],dtype=tf.int64)\n",
    "}\n",
    "\n",
    "context_parsed, sequence_parsed = tf.parse_single_sequence_example(\n",
    "    serialized=serialized_example,\n",
    "    context_features=context_features,\n",
    "    sequence_features=sequence_features\n",
    ")\n",
    "\n",
    "#batch_data = tf.train.batch(tensors=[sequence_parsed['feature']],batch_size=2,dynamic_pad=True,allow_smaller_final_batch=True)\n",
    "batch_label,batch_data = tf.train.batch(tensors=[context_parsed['label'], sequence_parsed['feature']],batch_size=2,dynamic_pad=True,allow_smaller_final_batch=True)\n",
    "# batch_label,batch_data = tf.train.batch(tensors=[context_parsed['label'], sequence_parsed['feature']],batch_size=2,allow_smaller_final_batch=True)\n",
    "print(batch_data)\n",
    "\n",
    "\n",
    "print(batch_label)\n",
    "\n",
    "# result = tf.contrib.learn.run_n({\"index\":batch_data})\n",
    "init_op = [\n",
    "      tf.global_variables_initializer(),\n",
    "      tf.local_variables_initializer()\n",
    "  ]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            print(sess.run([batch_label,batch_data]))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "         print(\"Done train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下主要是把不定长的key写入到tfrecord中，只不过不是按sequence example的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "labels=[2,3,5]\n",
    "keys=[[1,2],[2,5,6],[2,5,6,7,8,9]]\n",
    "\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def make_example(label,key):\n",
    "    example = tf.train.Example(features=tf.train.Features(\n",
    "        feature={\n",
    "            'label':tf.train.Feature(float_list=tf.train.FloatList(value=[label])),\n",
    "              'vu':tf.train.Feature(float_list=tf.train.FloatList(value=key))\n",
    "          \n",
    "        }\n",
    "    )\n",
    "                                )\n",
    "    return example.SerializeToString()\n",
    "\n",
    "\n",
    "filename=\"./tmp4.tfrecords\"\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "writer = tf.python_io.TFRecordWriter(filename)\n",
    "for k in range(len(keys)):\n",
    "    key=keys[k]\n",
    "    label=labels[k]\n",
    "    ex = make_example(label,key)\n",
    "    writer.write(ex)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下的主要是试验了VarLenFeature的用法，如果是tf.VarLenFeature(tf.float32)，会被解析成SparseTensor；这样是不能满足text cnn的要求的，textcnn 要求是一个句子同一个长度，这样可以解析成句子长度行*embeding的尺寸的列数\n",
    "\n",
    "如果key=tf.FixedLenFeature([], tf.float32)好像总会报错，因为key本来就不是定长的，按道理不会报错吧？？如果不会就可以满足text cnn 的要求，也即是每个句子的长度固定，不够的用0来补\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "reader = tf.TFRecordReader()\n",
    "filename_queue = tf.train.string_input_producer([\"./tmp4.tfrecords\"],num_epochs=3)\n",
    "_,batch_serialized_example =reader.read(filename_queue)\n",
    "\n",
    "# coord = tf.train.Coordinator()\n",
    "# threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "\n",
    "\n",
    "\n",
    "features = tf.parse_single_example(\n",
    "      batch_serialized_example,\n",
    "      features={\n",
    "          \"label\": tf.FixedLenFeature([], tf.float32),\n",
    "#           \"key\":tf.VarLenFeature(tf.float32)\n",
    "           \"vu\": tf.VarLenFeature(tf.float32)\n",
    "      })\n",
    "batch_labels = features[\"label\"]\n",
    "batch_values = features[\"vu\"]\n",
    "\n",
    "#batch_data = tf.train.batch(tensors=[sequence_parsed['feature']],batch_size=2,dynamic_pad=True,allow_smaller_final_batch=True)\n",
    "batch_label,batch_data = tf.train.batch(tensors=[batch_labels, batch_values],\n",
    "                                        batch_size=2)\n",
    "# batch_label,batch_data = tf.train.batch(tensors=[context_parsed['label'], sequence_parsed['feature']],batch_size=2,allow_smaller_final_batch=True)\n",
    "\n",
    "print(batch_data)\n",
    "\n",
    "\n",
    "print(batch_label)\n",
    "\n",
    "# result = tf.contrib.learn.run_n({\"index\":batch_data})\n",
    "init_op = [\n",
    "      tf.global_variables_initializer(),\n",
    "      tf.local_variables_initializer()\n",
    "  ]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            print(sess.run([batch_label,batch_data]))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "         print(\"Done train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding_lookup_sparsesparse\n",
    "这个主要演示了tf.nn.embedding_lookup_sparsesparse，sparse中的向量实际上是[[1\tnull\t2],[2\t0\tnull]],所以计算过程是先根据1对应embeding的向量（也即是a的第2行）[2 3]，然后根据2对应的embedding的向量（也即是a的第3行）[4 5]，然后相加得到sparse第0行的向量，在embedding查询后是[6. 8.]，sparse的第1行也类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "a = np.arange(8).reshape(4, 2)\n",
    "\n",
    "\n",
    "print(a)\n",
    "# print(b)\n",
    "# print(c)\n",
    "\n",
    "a = tf.Variable(a, dtype=tf.float32)\n",
    "\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "model_dir='C:/work/tensorflow_template/log/model.ckpt'\n",
    "checkpoint_path =model_dir \n",
    "reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path) \n",
    "print('22')\n",
    "var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "aa=reader.get_tensor('embeddings/Variable')\n",
    "for key in var_to_shape_map:  \n",
    "    print(\"tensor_name: \", key)  \n",
    "    print(reader.get_tensor(key)) #\n",
    "\n",
    "idx = tf.SparseTensor(indices=[[0, 0], [0, 2], [1, 0], [1, 1]], values=[1, 2, 2, 0], dense_shape=(2, 3))\n",
    "result = tf.nn.embedding_lookup_sparse(aa, idx, None, combiner=\"sum\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "id,result2 = sess.run([idx,result])\n",
    "print('---')\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "def input_fn(data_file):\n",
    "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
    "  assert tf.gfile.Exists(data_file), (\n",
    "      '%s not found. Please make sure you have either run data_download.py or '\n",
    "      'set both arguments --train_data and --test_data.' % data_file)\n",
    "\n",
    "  def parse_csv(value):\n",
    "    print('Parsing', data_file)\n",
    "    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "    features = dict(zip(_CSV_COLUMNS, columns))\n",
    "    \n",
    "    return features\n",
    "\n",
    "  # Extract lines from input files using the Dataset API.\n",
    "  dataset = tf.data.TextLineDataset(data_file)\n",
    "  dataset = dataset.map(parse_csv, num_parallel_calls=1)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_element\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "a=tf.placeholder(tf.float32,shape=(5, None))\n",
    "w = np.array([[1.0, 2.0, 3.0, 4.0, 5.0]])\n",
    "w=np.transpose(w)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(w)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(one_element,feed_dict={a:w}))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"end!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python import pywrap_tensorflow\n",
    "model_dir='C:\\work\\tensorflow_template\\log\\checkpoint\\model.ckpt'\n",
    "checkpoint_path =model_dir \n",
    "reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path) \n",
    "print('22')\n",
    "var_to_shape_map = reader.get_variable_to_shape_map()  \n",
    "for key in var_to_shape_map:  \n",
    "    print(\"tensor_name: \", key)  \n",
    "    print(reader.get_tensor(key)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open('./metadata.tsv', 'w','utf8') as f:\n",
    "    f.write('நியூஸ்7 தமிழ்')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "directory =[]\n",
    "for i in range(1, 555576, 10000):\n",
    "#     output_filename=\"C:/work/tensorflow_template2/tags_{}.tfrecords\".format(i)\n",
    "    output_filename=\"s3://shareit.tmp/tangguoping/tags/tags_{}.tfrecords\".format(i)\n",
    "    directory.append(output_filename)\n",
    "file_names = tf.train.match_filenames_once(directory)\n",
    "\n",
    "init = (tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(file_names))\n",
    "    \n",
    "    \n",
    "for i in range(5):\n",
    "    print(i)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('./metadata.tsv', 'r','utf8') as f:\n",
    "  all_line_txt = f.readlines()\n",
    "  for (i,l) in enumerate(all_line_txt):\n",
    "    print(l)\n",
    "import csv\n",
    "csv_reader = csv.reader(open('./metadata.tsv', encoding='utf-8'))\n",
    "for row in csv_reader:\n",
    "        print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# 新建一个Session\n",
    "with tf.Session() as sess:\n",
    "    # 我们要读三幅图片A.jpg, B.jpg, C.jpg\n",
    "    filename = ['A.jpg', 'B.jpg', 'C.jpg']\n",
    "    # string_input_producer会产生一个文件名队列\n",
    "    filename_queue = tf.train.string_input_producer(filename, shuffle=False, num_epochs=5)\n",
    "  \n",
    "    # tf.train.string_input_producer定义了一个epoch变量，要对它进行初始化\n",
    "    tf.local_variables_initializer().run()\n",
    "    # 使用start_queue_runners之后，才会开始填充队列\n",
    "    threads = tf.train.start_queue_runners(sess=sess)\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        # 获取图片数据并保存\n",
    "        print(sess.run(filename_queue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "c=tf.constant(value=1)\n",
    "print( c.graph ==tf.get_default_graph())\n",
    "print(c.graph)\n",
    "print(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph的含义，在with里面写的ops都属于那个图，不在的属于默认图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "c=tf.constant(value=1)\n",
    "print(c.graph == tf.get_default_graph())\n",
    "print(c.graph)\n",
    "print(tf.get_default_graph())\n",
    "\n",
    "g=tf.Graph()\n",
    "print(\"g:\",g)\n",
    "with g.as_default():\n",
    "    d=tf.constant(value=2)\n",
    "    print(\"d:\",d.graph)\n",
    "    #print(g)\n",
    "\n",
    "g2=tf.Graph()\n",
    "print(\"g2:\",g2)\n",
    "g2.as_default()\n",
    "e=tf.constant(value=15)\n",
    "print(\"e:\",e.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的例子里面创创建了一个新的图g，然后把g设为默认，那么接下来的操作不是在默认的图中，而是在g中了。你也可以认为现在g这个图就是新的默认的图了。\n",
    "要注意的是，最后一个量e不是定义在with语句里面的，也就是说，e会包含在最开始的那个图中。也就是说，要在某个graph里面定义量，要在with语句的范围里面定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & restore & continue train in tensorflow\n",
    "\n",
    "1.globalstep的主要作用是，例如想 多次训练模型，那么就有local step(指的是本次训练在哪一步)，和global_stepI（在所有训练中这个有属于哪一步）,globalstep在minimize时候更新:\n",
    "\n",
    "train = optimizer.minimize(loss,global_step=global_step)  \n",
    "\n",
    "2. saver.restore之后就可以用同一套代码接着训练了。这段demo，分2次运行，可以看到第一次w 从-0.5219252,b从0开始，结尾是w=5.7128973，b=8.272915，运行时候第二次w=5.7128973，b=8.272915开始因此，这个就是从第一次的结尾继续训练。\n",
    "\n",
    "global_step第二次变为9开始，因此这个也是从第一次接着往下，那么在summary 画图时候就额可以拼接多次训练的成果了。\n",
    "\n",
    "此外发现W = tf.Variable(tf.random_uniform([1], -1.0, 1.0)) ，加入seed=1之后，第二次还是从-0.5219252，奇怪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TangGuoping\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore session from checkpoint: C:/Users/TangGuoping/Desktop/check/checkpoint.ckpt-8\n",
      "INFO:tensorflow:Restoring parameters from C:/Users/TangGuoping/Desktop/check/checkpoint.ckpt-8\n",
      "before: this run is no.0 local step, is no.0 in global step,  W[6.290342] b[7.916188]\n",
      "after: this run is no.0 local step, is no.9 in global step,  W[6.376238] b[7.7697845]\n",
      "before: this run is no.1 local step, is no.9 in global step,  W[6.376238] b[7.7697845]\n",
      "after: this run is no.1 local step, is no.10 in global step,  W[6.5030117] b[7.731109]\n",
      "before: this run is no.2 local step, is no.10 in global step,  W[6.5030117] b[7.731109]\n",
      "after: this run is no.2 local step, is no.11 in global step,  W[6.610241] b[7.6740284]\n",
      "before: this run is no.3 local step, is no.11 in global step,  W[6.610241] b[7.6740284]\n",
      "after: this run is no.3 local step, is no.12 in global step,  W[6.7119107] b[7.6257477]\n",
      "before: this run is no.4 local step, is no.12 in global step,  W[6.7119107] b[7.6257477]\n",
      "after: this run is no.4 local step, is no.13 in global step,  W[6.80568] b[7.5799704]\n",
      "before: this run is no.5 local step, is no.13 in global step,  W[6.80568] b[7.5799704]\n",
      "after: this run is no.5 local step, is no.14 in global step,  W[6.892724] b[7.5377502]\n",
      "before: this run is no.6 local step, is no.14 in global step,  W[6.892724] b[7.5377502]\n",
      "after: this run is no.6 local step, is no.15 in global step,  W[6.973402] b[7.498558]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# 模拟生成100对数据对, 对应的函数为y = x * 0.1 + 0.3  \n",
    "x_data = np.random.rand(100).astype(\"float32\")  \n",
    "y_data = x_data * 8 + 7\n",
    "#记录在多次训练中，全局的步数\n",
    "global_step = tf.Variable(1, name=\"global_step\", trainable=False)\n",
    "\n",
    "# 指定w和b变量的取值范围（利用TensorFlow来得到w和b的值）  \n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0)) #随机生成一个在[-1,1]范围的均匀分布数值  \n",
    "b = tf.Variable(tf.zeros([1])) #set b=0  \n",
    "y = W * x_data + b \n",
    "\n",
    "# 最小化均方误差  \n",
    "loss = tf.reduce_mean(tf.square(y - y_data))  \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5) #<span style=\"font-size:14px;\">学习率为0.5的梯度下降法</span>  \n",
    "\n",
    "#除了常规的loss,还有global_step需要更新\n",
    "train = optimizer.minimize(loss,global_step=global_step)  \n",
    "\n",
    "saver = tf.train.Saver()\n",
    "checkpoint_path='C:/Users/TangGuoping/Desktop/check'\n",
    "CHECKPOINT_FILE = checkpoint_path + \"/checkpoint\"\n",
    "LATEST_CHECKPOINT = tf.train.latest_checkpoint(checkpoint_path)\n",
    "init_op = [\n",
    "      tf.global_variables_initializer(),\n",
    "      tf.local_variables_initializer()\n",
    "  ]\n",
    "def restore_from_checkpoint(sess, saver, checkpoint):\n",
    "  if checkpoint:\n",
    "    print(\"Restore session from checkpoint: {}\".format(checkpoint))\n",
    "    saver.restore(sess, checkpoint)\n",
    "    return True\n",
    "  else:\n",
    "    print(\"Checkpoint not found: {}\".format(checkpoint))\n",
    "    return False\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    restore_from_checkpoint(sess, saver, LATEST_CHECKPOINT)\n",
    "    gsp=0\n",
    "    #注意比较local step与global step\n",
    "    for step in range(7):\n",
    "        print('before: this run is no.{} local step, is no.{} in global step,  W{} b{}'.format(step,gsp, sess.run(W), sess.run(b)))\n",
    "        _,gsp=sess.run([train,global_step])  \n",
    "        print('after: this run is no.{} local step, is no.{} in global step,  W{} b{}'.format(step,gsp, sess.run(W), sess.run(b)))\n",
    "        if step%3==0:\n",
    "            saver.save(sess, CHECKPOINT_FILE, global_step=gsp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2after3\n"
     ]
    }
   ],
   "source": [
    "print('step %dafter%d'%(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for C:/Users/TangGuoping/Desktop/check/checkpoint.ckpt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c1723a254ae8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Users/TangGuoping/Desktop/check'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m\"/checkpoint.ckpt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mLATEST_CHECKPOINT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mLATEST_CHECKPOINT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mvar_to_shape_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable_to_shape_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[1;34m(filepattern)\u001b[0m\n\u001b[0;32m    252\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tf_api_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'train.NewCheckpointReader'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    514\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for C:/Users/TangGuoping/Desktop/check/checkpoint.ckpt"
     ]
    }
   ],
   "source": [
    "from tensorflow.python import pywrap_tensorflow  \n",
    "checkpoint_path = 'C:/Users/TangGuoping/Desktop/check'+ \"/checkpoint.ckpt\"\n",
    "LATEST_CHECKPOINT = tf.train.latest_checkpoint(checkpoint_path)\n",
    "reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path)\n",
    "LATEST_CHECKPOINT = tf.train.latest_checkpoint(checkpoint_path)\n",
    "var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "for key in var_to_shape_map:  \n",
    "    print(\"tensor_name: \", key)  \n",
    "    print(reader.get_tensor(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
